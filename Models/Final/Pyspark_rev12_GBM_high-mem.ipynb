{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.port.maxRetries', '128'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://hdp002.cac.queensu.ca:8088/proxy/application_1548786446322_4811'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'hdp001.cac.queensu.ca:18081'),\n",
       " ('spark.driver.port', '36691'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.app.id', 'application_1548786446322_4811'),\n",
       " ('spark.driver.cores', '1'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'hdp002.cac.queensu.ca'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.driver.host', 'hdp006'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip'),\n",
       " ('spark.app.name', 'Spark_GBM'),\n",
       " ('spark.driver.appUIAddress', 'http://hdp006:4041'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.am.cores', '1')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark_GBM\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '95g'), ('spark.app.name', 'Spark_GBM'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','95g')])\n",
    "#Stop the current Spark Session\n",
    "\n",
    "spark.sparkContext.stop()\n",
    "#Create a Spark Session\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/user/mie_sbetancourt/PROJECT/Data/data_reduced_reweighted_FINAL_3.csv\"\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "\n",
    "# Import the data into a Spark DataFrame with the schema \n",
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(file_location)\n",
    "\n",
    "\n",
    "#data = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "data = (data.drop(\"Census_FirmwareVersionIdentifier\")\n",
    "        .withColumn(\"AVProductsEnabled\", when(data[\"AVProductsEnabled\"]==\"unknown\", -1).otherwise(data[\"AVProductsEnabled\"])))\n",
    "    #.withColumn(\"classWeightCol\", when(data[\"classWeightCol\"]>.5, 0.091).otherwise(0.908)))\n",
    "\n",
    "data_1 = (data.withColumn(\"AVProductsEnabled\", data[\"AVProductsEnabled\"].cast(IntegerType()))\n",
    "          .withColumn(\"AvSigVersion_new\", data[\"AvSigVersion_new\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSBuildNumber\", data[\"Census_OSBuildNumber\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSBuildRevision\", data[\"Census_OSBuildRevision\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSUILocaleIdentifier\", data[\"Census_OSUILocaleIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSVersion_new\", data[\"Census_OSVersion_new\"].cast(StringType()))\n",
    "          .withColumn(\"CountryIdentifier\", data[\"CountryIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"LocaleEnglishNameIdentifier\", data[\"LocaleEnglishNameIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"OsBuild\", data[\"OsBuild\"].cast(StringType()))\n",
    "          .withColumn(\"OsSuite\", data[\"OsSuite\"].cast(StringType())))\n",
    "data_1 = data_1.withColumnRenamed(\"HasDetections\",\"label\").drop(\"OsBuildLab_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringCols = []\n",
    "for col in data_1.dtypes:\n",
    "    if col[1] == 'string':\n",
    "        stringCols.append(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MachineIdentifier'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringCols.pop(0)\n",
    "#stringCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numericCols = np.setdiff1d(data_1.columns, stringCols).tolist()\n",
    "numericCols.remove(\"MachineIdentifier\")\n",
    "numericCols.remove(\"classWeightCol\")\n",
    "numericCols.remove(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_seed=1111\n",
    "trainingData1 = data_1.sampleBy(\"label\", fractions={0: .1, 1: 1}, seed=sampling_seed)\n",
    "trainingData = trainingData1.sampleBy(\"label\", fractions={0: .9, 1: .9}, seed=sampling_seed)\n",
    "# Subtracting 'train' from original 'data' to get test set \n",
    "testData = trainingData1.subtract(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|401788|\n",
      "|    0|402132|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/user/mie_sbetancourt/PROJECT/\"\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"keep\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "gbt = (GBTClassifier(labelCol=\"label\", featuresCol=\"features\"))\n",
    "      \n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "      \n",
    "#pipeline = Pipeline(stages=indexers + encoders+[assembler]+lr)\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[gbt])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [4, 6, 8]) \\\n",
    "    .addGrid(gbt.maxBins, [15, 25])  \\\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.05]) \\\n",
    "    .addGrid(gbt.subsamplingRate, [.7]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "gbt_crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "gbt_cvmodel = gbt_crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "gbt_path = temp_path + \"/gbt\"\n",
    "gbt.save(gbt_path)\n",
    "model_path = temp_path + \"/gbt_model\"\n",
    "gbt_cvmodel.bestModel.save(model_path)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_path = temp_path + \"/gbt\"\n",
    "gbt.save(gbt_path)\n",
    "model_path = temp_path + \"/gbt_model\"\n",
    "gbt_cvmodel.bestModel.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC for train set after CV  is 0.689483203950502\n",
      "The area under ROC for test set after CV  is 0.657841253989691\n",
      "Best maxDepth:  8\n",
      "Best maxBins:  25\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import *\n",
    "gbt_model_path = temp_path + \"/gbt_model\"\n",
    "gbt_mod2 = PipelineModel.load(gbt_model_path)\n",
    "gbt_predictions = gbt_mod2.transform(testData)\n",
    "gbt_train_predictions = gbt_mod2.transform(trainingData)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(gbt_train_predictions)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(gbt_predictions)))\n",
    "print ('Best maxDepth: ', gbt_mod2.stages[-1]._java_obj.getMaxDepth()\n",
    "print ('Best maxBins: ', gbt_mod2.stages[-1]._java_obj.getMaxBins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best stepSize:  0.1\n"
     ]
    }
   ],
   "source": [
    "print('Best stepSize: ', gbt_mod2.stages[-1]._java_obj.getStepSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1st try ---- 4 hours\n",
    "\n",
    "gbt.maxDepth, [4, 8, 12]) \\\n",
    "    .addGrid(gbt.maxBins, [20, 25])  \\\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.05]) \\\n",
    "    \n",
    "The area under ROC for train set after CV  is 0.689483203950502\n",
    "The area under ROC for test set after CV  is 0.657841253989691\n",
    "Best maxDepth:  8\n",
    "Best maxBins:  25\n",
    "Best stepSize:  0.1    \n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2nd place lightGBM\n",
    "best_hyp = {'boosting_type': 'gbdt',\n",
    "            'class_weight': None,\n",
    "            'colsample_bytree': 0.2685319471585845,\n",
    "            'learning_rate': 0.011114918157721438,\n",
    "            'min_child_samples': 270,\n",
    "            'num_leaves': 261,\n",
    "            'reg_alpha': 0.4182767807212193,\n",
    "            'reg_lambda': 0.07336659149142766,\n",
    "            'subsample_for_bin': 40000,\n",
    "            'subsample': 0.6462594904717385}\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Pyspark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
