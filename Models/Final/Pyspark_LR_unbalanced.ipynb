{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 750.47998046875,
      "end_time": 1561221957810.437
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1548786446322_4794'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.executor.memory', '5g'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.app.name', 'Spark_LR_65g'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.port.maxRetries', '128'),\n",
       " ('spark.app.id', 'application_1548786446322_4794'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'hdp001.cac.queensu.ca:18081'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.driver.cores', '1'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'hdp002.cac.queensu.ca'),\n",
       " ('spark.driver.appUIAddress', 'http://hdp006:4046'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://hdp002.cac.queensu.ca:8088/proxy/application_1548786446322_4794'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.driver.host', 'hdp006'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.driver.port', '43236'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.am.cores', '1')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark_LR_65g\").getOrCreate()\n",
    "spark.sparkContext._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Update the default configurations\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '65g'), ('spark.app.name', 'LR_unbalanced'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','65g')])\n",
    "#Stop the current Spark Session\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "#Create a Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Update the default configurations\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '5g'), ('spark.app.name', 'LR_imbalanced'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','5g')])\n",
    "#Stop the current Spark Session\n",
    "spark.sparkContext.stop()\n",
    "\n",
    "#Create a Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1548786446322_4794'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.executor.memory', '65g'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.app.name', 'LR_unbalanced'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.port.maxRetries', '128'),\n",
       " ('spark.app.id', 'application_1548786446322_4794'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.yarn.historyServer.address', 'hdp001.cac.queensu.ca:18081'),\n",
       " ('spark.driver.memory', '65g'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.driver.cores', '1'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'hdp002.cac.queensu.ca'),\n",
       " ('spark.driver.appUIAddress', 'http://hdp006:4046'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://hdp002.cac.queensu.ca:8088/proxy/application_1548786446322_4794'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.driver.host', 'hdp006'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.driver.port', '43236'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/hdp/current/spark2-client/python:/usr/hdp/current/spark2-client/python/lib:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.am.cores', '1')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 49.416015625,
      "end_time": 1561221957045.628
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Queen's cluster\n",
    "#https://uoft.cac.queensu.ca/\n",
    "\n",
    "# LARGE, IMBALANCED dataset\n",
    "#file_location = \"data_reduced_reweighted_final_3_csv\"\n",
    "file_location = \"/user/mie_sbetancourt/PROJECT/Data/data_reduced_reweighted_FINAL_3.csv\"\n",
    "#print(file_location)\n",
    "# SMALL, BALANCED dataset\n",
    "#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv/part-00000-0b2544e2-78ed-44d9-808d-1d2e4ec6a8df-c000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35614.75,
      "end_time": 1561221993438.689
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "#data = spark.table(\"data_reduced_reweighted_final_3_csv\")\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SVC_1').getOrCreate()\n",
    "\n",
    "# Import the data into a Spark DataFrame with the schema \n",
    "data = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(file_location)\n",
    "\n",
    "\n",
    "#data = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "data = (data.drop(\"Census_FirmwareVersionIdentifier\")\n",
    "        .withColumn(\"AVProductsEnabled\", when(data[\"AVProductsEnabled\"]==\"unknown\", -1).otherwise(data[\"AVProductsEnabled\"]))\n",
    "        .withColumn(\"classWeightCol\", when(data[\"classWeightCol\"]>.5, 0.091).otherwise(0.908)))\n",
    "\n",
    "data_1 = (data.withColumn(\"AVProductsEnabled\", data[\"AVProductsEnabled\"].cast(IntegerType()))\n",
    "          .withColumn(\"AvSigVersion_new\", data[\"AvSigVersion_new\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSBuildNumber\", data[\"Census_OSBuildNumber\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSBuildRevision\", data[\"Census_OSBuildRevision\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSUILocaleIdentifier\", data[\"Census_OSUILocaleIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"Census_OSVersion_new\", data[\"Census_OSVersion_new\"].cast(StringType()))\n",
    "          .withColumn(\"CountryIdentifier\", data[\"CountryIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"LocaleEnglishNameIdentifier\", data[\"LocaleEnglishNameIdentifier\"].cast(StringType()))\n",
    "          .withColumn(\"OsBuild\", data[\"OsBuild\"].cast(StringType()))\n",
    "          .withColumn(\"OsSuite\", data[\"OsSuite\"].cast(StringType())))\n",
    "data_1 = data_1.withColumnRenamed(\"HasDetections\",\"label\").drop(\"OsBuildLab_new\")\n",
    "#sampling_seed=1111\n",
    "#data = data_3.sampleBy(\"HasDetections\", fractions={0: .025, 1: .25}, seed=sampling_seed)\n",
    "#data_reduced.groupBy(\"HasDetections\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.7490234375,
      "end_time": 1561221993486.065
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_reduced.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 34.31591796875,
      "end_time": 1561221993548.922
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_reduced_reweighted_final_3.csv/part-00000-54f91216-357c-4161-9d25-ee9577740a9c-c000.csv\"\n",
    "\n",
    "#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv/part-00000-0b2544e2-78ed-44d9-808d-1d2e4ec6a8df-c000.csv\"\n",
    "\n",
    "#data = spark.read.csv(file_location, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.755859375,
      "end_time": 1561221993816.972
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 54.006103515625,
      "end_time": 1561221993884.949
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_1.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 37.989990234375,
      "end_time": 1561221993941.847
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "### Number of nulls or NAN in each column\n",
    "#numOfnulls = data_1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data_1.columns])\n",
    "### View the number of nulls in each column in a Pandas DataFrame format\n",
    "#numOfnulls_pandas = numOfnulls.toPandas()\n",
    "#numOfnulls_pandas.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 238.85693359375,
      "end_time": 1560960831272.571
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 38.802001953125,
      "end_time": 1561221993992.408
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function drops columns which have more than 10% of their entries as null values\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def drop_null_columns(df):\n",
    "    \"\"\"\n",
    "    This function drops columns which have more than 10% of their entries as null values.\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = df.select([F.count(F.when(isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v > 0.1*(df.count())]\n",
    "    df = df.drop(*to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 34.29296875,
      "end_time": 1561221994036.945
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the colums with more than 10% of the records containing and create a new DataFrame \"data_1\"\n",
    "#data_1 = drop_null_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 39.741943359375,
      "end_time": 1561221994094.644
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of columns remaining in DF \"data_1\"\n",
    "#len(data_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 37.12109375,
      "end_time": 1561221994145.789
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Check the number of unique values in each column of the dataframe \"data_1\" and view in a Pandas DataFrame format\n",
    "#numUniqueVals = data_1.agg(*(F.countDistinct(col(c)).alias(c) for c in data_1.columns))\n",
    "#numUniqueVals_pandas = numUniqueVals.toPandas()\n",
    "#numUniqueVals_pandas.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31.889892578125,
      "end_time": 1561221994192.066
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Create a list of columns which have less than 20 unique values\n",
    "#numUniqueVals_pandas.T\n",
    "# Cols_with_less_than_20Unique_vals = [col for col, val in numUniqueVals_pandas.iteritems() if numUniqueVals_pandas[col][0]<20 ]\n",
    "# Cols_with_less_than_20Unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 37.508056640625,
      "end_time": 1561221994240.338
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new DF \"data_2\" which contains the columns with less than 20 unique values only\n",
    "#data_2 = data_1.select(Cols_with_less_than_20Unique_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 39.764892578125,
      "end_time": 1561221994297.953
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of columns in DF \"data_2\"\n",
    "#len(data_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 46.09521484375,
      "end_time": 1561221994363.775
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check: Check that the number of samples (rows) has not changed from the original dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.13916015625,
      "end_time": 1561221994621.651
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the rows (records) with NA entries in any of the X columns of \"data_2\" and save into a new DF \"data_3\"\n",
    "#data_1 = data_1.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.22802734375,
      "end_time": 1561221994665.777
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the number of rows remaining in DF \"data_1\"\n",
    "#data_1.select(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.444091796875,
      "end_time": 1561221994710.558
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a sample of the data to look at in a Pandas dataframe (the sample contains 5% of the full data)\n",
    "# data_sample = data_3.sample(False, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 238.368896484375,
      "end_time": 1561221994961.444
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the column containing the labels from the features dataframe before vectorizing\n",
    "data_1b = data_1.drop(\"label\").drop(\"MachineIdentifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 34.00390625,
      "end_time": 1560990139813.923
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1| 447000|\n",
      "|    0|4462591|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 32.5498046875,
      "end_time": 1561221995004.394
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the data types of the columns in DF \"data_3b\"\n",
    "#data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.630126953125,
      "end_time": 1561221995046.155
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stringCols = []\n",
    "for col in data_1.dtypes:\n",
    "    if col[1] == 'string':\n",
    "        stringCols.append(col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.85009765625,
      "end_time": 1561221995087.413
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MachineIdentifier'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringCols.pop(0)\n",
    "#stringCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.22802734375,
      "end_time": 1561221995127.935
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numericCols = np.setdiff1d(data_1.columns, stringCols).tolist()\n",
    "numericCols.remove(\"MachineIdentifier\")\n",
    "numericCols.remove(\"classWeightCol\")\n",
    "numericCols.remove(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31.218994140625,
      "end_time": 1561221995170.6
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 744.26806640625,
      "end_time": 1561221995924.96
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_seed=1111\n",
    "trainingData1 = data_1.sampleBy(\"label\", fractions={0: .05, 1: .05}, seed=sampling_seed)\n",
    "trainingData = trainingData1.sampleBy(\"label\", fractions={0: .9, 1: .9}, seed=sampling_seed)\n",
    "# Subtracting 'train' from original 'data' to get test set \n",
    "testData = trainingData1.subtract(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 21343.81005859375,
      "end_time": 1561212704408.246
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1| 20030|\n",
      "|    0|201404|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#trainingData.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MachineIdentifier</th>\n",
       "      <th>EngineVersion</th>\n",
       "      <th>AppVersion</th>\n",
       "      <th>RtpStateBitfield</th>\n",
       "      <th>IsSxsPassiveMode</th>\n",
       "      <th>AVProductStatesIdentifier</th>\n",
       "      <th>AVProductsInstalled</th>\n",
       "      <th>AVProductsEnabled</th>\n",
       "      <th>HasTpm</th>\n",
       "      <th>CountryIdentifier</th>\n",
       "      <th>...</th>\n",
       "      <th>Census_IsTouchEnabled</th>\n",
       "      <th>Census_IsPenCapable</th>\n",
       "      <th>Census_IsAlwaysOnAlwaysConnectedCapable</th>\n",
       "      <th>Wdft_IsGamer</th>\n",
       "      <th>Wdft_RegionIdentifier</th>\n",
       "      <th>label</th>\n",
       "      <th>classWeightCol</th>\n",
       "      <th>AvSigVersion_new</th>\n",
       "      <th>OSVer_new</th>\n",
       "      <th>Census_OSVersion_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231532ebd55348282b48b195d3dcd15a</td>\n",
       "      <td>1.1.15200.1</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.275</td>\n",
       "      <td>10.0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e7a3deed3dc23d01889d1c96f27720f</td>\n",
       "      <td>1.1.15300.6</td>\n",
       "      <td>4.18.1807.18075</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.277</td>\n",
       "      <td>10.0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a75b54fa4f54549ebac056b6ce018ba7</td>\n",
       "      <td>1.1.15200.1</td>\n",
       "      <td>4.9.10586.1106</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>201</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.275</td>\n",
       "      <td>10.0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f94a45006a152df8faa8159aacea3184</td>\n",
       "      <td>1.1.14104.0</td>\n",
       "      <td>4.12.16299.15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.251</td>\n",
       "      <td>10.0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>794efec320642a02b18719de96138846</td>\n",
       "      <td>1.1.15100.1</td>\n",
       "      <td>4.9.10586.1106</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>53447</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.273</td>\n",
       "      <td>10.0.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MachineIdentifier EngineVersion       AppVersion  \\\n",
       "0  231532ebd55348282b48b195d3dcd15a   1.1.15200.1  4.18.1807.18075   \n",
       "1  0e7a3deed3dc23d01889d1c96f27720f   1.1.15300.6  4.18.1807.18075   \n",
       "2  a75b54fa4f54549ebac056b6ce018ba7   1.1.15200.1   4.9.10586.1106   \n",
       "3  f94a45006a152df8faa8159aacea3184   1.1.14104.0    4.12.16299.15   \n",
       "4  794efec320642a02b18719de96138846   1.1.15100.1   4.9.10586.1106   \n",
       "\n",
       "  RtpStateBitfield  IsSxsPassiveMode AVProductStatesIdentifier  \\\n",
       "0                7                 0                     53447   \n",
       "1                7                 0                     53447   \n",
       "2                7                 0                     53447   \n",
       "3                7                 0                     53447   \n",
       "4                7                 0                     53447   \n",
       "\n",
       "   AVProductsInstalled  AVProductsEnabled  HasTpm CountryIdentifier  \\\n",
       "0                    1                  1       1                66   \n",
       "1                    1                  1       1               101   \n",
       "2                    1                  1       1               201   \n",
       "3                    1                  1       1                80   \n",
       "4                    1                  1       1                29   \n",
       "\n",
       "          ...          Census_IsTouchEnabled Census_IsPenCapable  \\\n",
       "0         ...                              0                   0   \n",
       "1         ...                              0                   0   \n",
       "2         ...                              0                   0   \n",
       "3         ...                              1                   0   \n",
       "4         ...                              0                   0   \n",
       "\n",
       "  Census_IsAlwaysOnAlwaysConnectedCapable Wdft_IsGamer Wdft_RegionIdentifier  \\\n",
       "0                                       0            1                     5   \n",
       "1                                       0            0                     9   \n",
       "2                                       0            0                    11   \n",
       "3                                       1            0                     3   \n",
       "4                                       0            1                    10   \n",
       "\n",
       "  label classWeightCol AvSigVersion_new OSVer_new Census_OSVersion_new  \n",
       "0     0          0.091            1.275    10.0.0                 10.0  \n",
       "1     0          0.091            1.277    10.0.0                 10.0  \n",
       "2     0          0.091            1.275    10.0.0                 10.0  \n",
       "3     0          0.091            1.251    10.0.0                 10.0  \n",
       "4     0          0.091            1.273    10.0.0                 10.0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "viewer = trainingData.toPandas()\n",
    "viewer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 242.44677734375,
      "end_time": 1561129520573.308
     }
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1985563.0629882812,
      "end_time": 1561131506151.064
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/python/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-2b77853cb6d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m                          seed=sampling_seed)\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mcvmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;31m# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# is areaUnderROC.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/python/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('LR_1').getOrCreate()\n",
    "\n",
    "# Import libraries\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"skip\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "lr = (LogisticRegression(labelCol=\"label\", featuresCol=\"features\",weightCol=\"classWeightCol\"))\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "      \n",
    "#pipeline = Pipeline(stages=indexers + encoders+[assembler]+lr)\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[lr])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, .025, .1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, .2,.8, 1.0])  \\\n",
    "    .addGrid(lr.fitIntercept,[True,False])  \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "cvmodel = crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 71665.10595703125,
      "end_time": 1561131953221.166
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "predictions = cvmodel.transform(testData)\n",
    "train_predictions = cvmodel.transform(trainingData)\n",
    "#evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 91984.98803710938,
      "end_time": 1561136488083.298
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(train_predictions)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(predictions)))\n",
    "print ('Best Param (regParam): ', cvmodel.bestModel.stages[-1]._java_obj.parent().getRegParam())\n",
    "print ('Best ElasticParam: ', cvmodel.bestModel.stages[-1]._java_obj.parent().getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1st try on Queens\n",
    "900K training\n",
    "--- 9758.53647518158 seconds --- 3 hrs\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.0, .025, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, .5, 1.0])  \\\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "The area under ROC for train set after CV  is 0.6766547075808887\n",
    "The area under ROC for test set after CV  is 0.6709478670932243\n",
    "Best Param (regParam):  0.025\n",
    "Best ElasticParam:  0.0\n",
    "                         \n",
    "2nd try on Queens\n",
    "200K training                         \n",
    "--- 4215.349386930466 seconds --- 70 mins\n",
    "The area under ROC for train set after CV  is 0.6829531111489988\n",
    "The area under ROC for test set after CV  is 0.6630408598358555\n",
    "Best Param (regParam):  0.025\n",
    "Best ElasticParam:  0.0\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 241.123046875,
      "end_time": 1561136143577.846
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotting = predictions.select(\"label\", \"prediction\", \"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 786.526123046875,
      "end_time": 1561137238142.974
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting the ROC Curve\n",
    "trainingSummary = cvmodel.bestModel.stages[-1].summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot([0,1],'r--')\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print ('Best Param (regParam): ', cvmodel.bestModel.stages[-1]._java_obj.parent().getRegParam())\n",
    "print ('Best ElasticParam: ', cvmodel.bestModel.stages[-1]._java_obj.parent().getElasticNetParam())\n",
    "print(\"Test set AUROC: {}\".format(evaluator.evaluate(predictions)))\n",
    "print('Training set AUROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SVM_1').getOrCreate()\n",
    "\n",
    "# Import libraries\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"skip\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "lsvc = (LinearSVC(labelCol=\"label\", featuresCol=\"features\"))\n",
    "      #,weightCol=\"classWeightCol\")) maxIter=100)) #, regParam=0.1, elasticNetParam=0.5))\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "      \n",
    "#pipeline = Pipeline(stages=indexers + encoders+[assembler]+lr)\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[lsvc])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(lsvc.regParam, [0.0, .025, .1]) \\\n",
    "    .addGrid(lsvc.fitIntercept, [True, False])  \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "lsvc_cvmodel = crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "lsvc_path = temp_path + \"/lsvc\"\n",
    "lsvc.save(lsvc_path)\n",
    "model_path = temp_path + \"/lsvc_model\"\n",
    "lsvc_cvmodel.bestModel.save(model_path)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Loading model\n",
    "from pyspark.ml import *\n",
    "lsvc_model_path = temp_path + \"/lsvc_model\"\n",
    "lsvc_mod2 = PipelineModel.load(lsvc_model_path)\n",
    "lsvc_predictions = lsvc_mod2.transform(testData)\n",
    "lsvc_train_predictions = lsvc_mod2.transform(trainingData)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(lsvc_train_predictions)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(lsvc_predictions)))\n",
    "print ('Best maxDepth: ', lsvc_mod2.stages[-1]._java_obj.getMaxDepth()\n",
    "print ('Best maxBins: ', lsvc_mod2.stages[-1]._java_obj.getMaxBins())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Split\n",
    "\n",
    "Would use in imbalanced case but too computationally expensive..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 163256.1728515625,
      "end_time": 1561062431068.648
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import since, keyword_only\n",
    "from pyspark.ml import Estimator, Model\n",
    "from pyspark.ml.common import _py2java\n",
    "from pyspark.ml.param import Params, Param, TypeConverters\n",
    "from pyspark.ml.param.shared import HasSeed\n",
    "from pyspark.ml.tuning import CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.util import *\n",
    "from pyspark.ml.wrapper import JavaParams\n",
    "from pyspark.sql.functions import rand\n",
    "from functools import reduce\n",
    "\n",
    "class StratifiedCrossValidator(CrossValidator):\n",
    "  def stratify_data(self, dataset):\n",
    "    \"\"\"\n",
    "    Returns an array of dataframes with the same ratio of passes and failures.\n",
    "    Currently only supports binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "    numModels = len(epm)\n",
    "    nFolds = self.getOrDefault(self.numFolds)\n",
    "    split_ratio = 1.0 / nFolds\n",
    "\n",
    "    passes = dataset[dataset['label'] == 1]\n",
    "    fails = dataset[dataset['label'] == 0]\n",
    "\n",
    "    pass_splits = passes.randomSplit([split_ratio for i in range(nFolds)])\n",
    "    fail_splits = fails.randomSplit([split_ratio for i in range(nFolds)])\n",
    "\n",
    "    stratified_data = [pass_splits[i].unionAll(fail_splits[i]) for i in range(nFolds)]\n",
    "\n",
    "    return stratified_data\n",
    "\n",
    "  def _fit(self, dataset):\n",
    "    est = self.getOrDefault(self.estimator)\n",
    "    epm = self.getOrDefault(self.estimatorParamMaps)\n",
    "    numModels = len(epm)\n",
    "    eva = self.getOrDefault(self.evaluator)\n",
    "    nFolds = self.getOrDefault(self.numFolds)\n",
    "    seed = self.getOrDefault(self.seed)\n",
    "    metrics = [0.0] * numModels\n",
    "\n",
    "    stratified_data = self.stratify_data(dataset)\n",
    "\n",
    "    for i in range(nFolds):\n",
    "      train_arr = [x for j,x in enumerate(stratified_data) if j != i]\n",
    "      train = reduce((lambda x, y: x.unionAll(y)), train_arr)\n",
    "      validation = stratified_data[i]\n",
    "\n",
    "      models = est.fit(train, epm)\n",
    "\n",
    "      for j in range(numModels):\n",
    "        model = models[j]\n",
    "        metric = eva.evaluate(model.transform(validation, epm[j]))\n",
    "        metrics[j] += metric/nFolds\n",
    "\n",
    "    if eva.isLargerBetter():\n",
    "      bestIndex = np.argmax(metrics)\n",
    "    else:\n",
    "      bestIndex = np.argmin(metrics)\n",
    "\n",
    "    bestModel = est.fit(dataset, epm[bestIndex])\n",
    "    return self._copyValues(CrossValidatorModel(bestModel, metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName(\"lightGBM\") \\\n",
    "#            .config(\"spark.jars.packages\", \"Azure:mmlspark:0.17\") \\\n",
    "#            .getOrCreate()\n",
    "#\n",
    "#import mmlspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3609488.2490234375,
      "end_time": 1561212440714.061
     }
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark GBT\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
    "\n",
    "temp_path = \"/user/mie_sbetancourt/PROJECT/\"\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"keep\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "gbt = (GBTClassifier(labelCol=\"label\", featuresCol=\"features\"))\n",
    "      \n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "      \n",
    "#pipeline = Pipeline(stages=indexers + encoders+[assembler]+lr)\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[gbt])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [4, 8]) \\\n",
    "    .addGrid(gbt.maxBins, [10, 25])  \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "gbt_crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "gbt_cvmodel = gbt_crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "gbt_path = temp_path + \"/gbt\"\n",
    "gbt.save(lr_path)\n",
    "model_path = temp_path + \"/gbt_model\"\n",
    "gbt_cvmodel.bestModel.save(model_path)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 15044.991943359375,
      "end_time": 1561212571667.648
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Loading model\n",
    "from pyspark.ml import *\n",
    "gbt_model_path = temp_path + \"/gbt_model\"\n",
    "gbt_mod2 = PipelineModel.load(gbt_model_path)\n",
    "gbt_predictions = gbt_mod2.transform(testData)\n",
    "gbt_train_predictions = gbt_mod2.transform(trainingData)\n",
    "print(\"The area under ROC for train set after CV  is {}\".format(evaluator.evaluate(gbt_train_predictions)))\n",
    "print(\"The area under ROC for test set after CV  is {}\".format(evaluator.evaluate(gbt_predictions)))\n",
    "print ('Best maxDepth: ', gbt_mod2.stages[-1]._java_obj.getMaxDepth()\n",
    "print ('Best maxBins: ', gbt_mod2.stages[-1]._java_obj.getMaxBins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting the ROC Curve\n",
    "gbt_trainingSummary = gbt_cvmodel.bestModel.stages[-1].summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "plt.plot([0,1],'r--')\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print ('Best maxBin: ', gbt_cvmodel.bestModel.stages[-1]._java_obj.parent().getMaxBins())\n",
    "print ('Best maxDepth: ', gbt_cvmodel.bestModel.stages[-1]._java_obj.parent().getMaxDepth())\n",
    "print(\"Test set AUROC: {}\".format(evaluator.evaluate(gbt_predictions)))\n",
    "print('Training set AUROC: ' + str(gbt_trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-001d478b545d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_path' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2065781.0629882812,
      "end_time": 1561227739064.676
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark RandFor\").getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
    "\n",
    "temp_path = \"/user/mie_sbetancourt/PROJECT/\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "#from FeatureImportanceSelector import ExtractFeatureImp, FeatureImpSelector\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"keep\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "randfor = (RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\"))\n",
    "      \n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "\n",
    "#labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                   labels=labelIndexer.labels)\n",
    "\n",
    "#pipeline = Pipeline(stages=indexers + encoders+[assembler]+lr)\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[randfor])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(randfor.numTrees, [150,200,250,300]) \\\n",
    "    .addGrid(randfor.maxDepth, [10,20,30]) \\\n",
    "    .addGrid(randfor.maxBins, [20]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "rand_cvmodel = crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "rand_path = temp_path + \"/rand\"\n",
    "randfor.save(rand_path)\n",
    "model_path = temp_path + \"/rand_model\"\n",
    "rand_cvmodel.bestModel.save(model_path)\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_path = temp_path + \"/rand\"\n",
    "randfor.save(rand_path)\n",
    "model_path = temp_path + \"/rand_model\"\n",
    "rand_cvmodel.bestModel.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11294.64990234375,
      "end_time": 1561224595517.728
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tried more trees but couldn't handle it\n",
    "#above took 44 mins\n",
    "rand_predictions = rand_cvmodel.transform(testData)\n",
    "rand_train_predictions = rand_cvmodel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 73779.23193359375,
      "end_time": 1561225180237.217
     }
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best numTrees:  150\n",
      "Best maxBins:  20\n",
      "Best maxDepth:  10\n",
      "Test set AUROC: 0.6551282170875691\n",
      "Training set AUROC: 0.6466502513935741\n"
     ]
    }
   ],
   "source": [
    "#rand_feat_imp = rand_cvmodel.bestModel.stages[-1].featureImportances\n",
    "print ('Best numTrees: ', rand_cvmodel.bestModel.stages[-1]._java_obj.parent().getNumTrees())\n",
    "print ('Best maxBins: ', rand_cvmodel.bestModel.stages[-1]._java_obj.parent().getMaxBins())\n",
    "print ('Best maxDepth: ', rand_cvmodel.bestModel.stages[-1]._java_obj.parent().getMaxDepth())\n",
    "print(\"Test set AUROC: {}\".format(evaluator.evaluate(rand_train_predictions)))\n",
    "print('Training set AUROC: {}'.format(evaluator.evaluate(rand_predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import *\n",
    "rand_model_path = temp_path + \"/rand_model\"\n",
    "rand_mod2 = PipelineModel.load(rand_model_path)\n",
    "rand_predictions = rand_mod2.transform(testData)\n",
    "rand_train_predictions = rand_mod2.transform(trainingData)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(rand_train_predictions)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(rand_predictions)))\n",
    "print ('Best numTrees: ', rand_mod2.stages[-1]._java_obj.getMaxDepth()\n",
    "print ('Best maxBins: ', rand_mod2.stages[-1]._java_obj.getMaxBins())\n",
    "print ('Best maxDepth: ', rand_mod2.stages[-1]._java_obj.getMaxDepth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2260.2939453125,
      "end_time": 1561224881497.836
     }
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "3-cross val\n",
    "\n",
    "1st try (Azure)\n",
    "2296.3062093257904 secs\n",
    "maxDepth:  8, 10\n",
    "numTrees: 8, 10\n",
    "Best maxDepth:  10\n",
    "Best numTrees: 10\n",
    "Test set AUROC: 0.6244246230461733\n",
    "Training set AUROC: 0.6263909435756815\n",
    "\n",
    "\n",
    "2nd try (Queens)\n",
    "--- 6261.523066282272 seconds ---\n",
    "randfor.numTrees, [10, 15]) \\\n",
    "(randfor.maxDepth, [3, 4, 5])\n",
    "Best numTrees:  15\n",
    "Best maxDepth:  5\n",
    "Training set AUROC: 0.6210402526931081\n",
    "Test set AUROC: 0.6256766022080265\n",
    "\n",
    "\n",
    "3rd try (Queens)\n",
    ".addGrid(randfor.numTrees, [50,100]) \\\n",
    "Best numTrees:  100\n",
    "Best maxBin:  20\n",
    "Best maxDepth:  7\n",
    "Test set AUROC: 0.6433332978783131\n",
    "Training set AUROC: 0.6407116809380157\n",
    "\n",
    "\n",
    "4th try (Queens)\n",
    "addGrid(randfor.numTrees, [150,200]) \\\n",
    "Best numTrees:  150\n",
    "Best maxBins:  20\n",
    "Best maxDepth:  10\n",
    "Test set AUROC: 0.6551282170875691\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9050,[0,1,2,3,4,5,6,7,11,14,15,16,61,62,66,67,70,72,73,77,79,86,87,94,161,163,168,169,170,171,174,175,179,181,182,183,187,188,189,191,192,194,198,201,205,226,232,280,296,297,309,319,341,360,370,409,576,644,656,715,870,916,1143,1568,2055,4925,7410,7413,7417,7419,7421,7422,7423,7424,7427,7438,7443,7445,7451,7455,7461,7468,7474,7478,7482,7483,7487,7490,7491,7518,7524,7631,7633,7634,7635,7637,7642,7643,7645,7653,7664,7669,7672,7682,7684,7694,7697,7699,7703,7773,7883,7888,7889,7892,7893,7896,7897,7898,7899,7911,7946,7947,7961,7963,7964,7969,7970,7977,7978,7979,7983,7988,7990,7992,8001,8006,8008,8012,8014,8020,8022,8024,8032,8039,8041,8042,8046,8047,8049,8061,8062,8132,8133,8136,8138,8139,8140,8141,8142,8145,8148,8152,8156,8157,8158,8159,8162,8168,8172,8188,8194,8252,8257,8263,8302,8307,8370,8372,8390,8391,8392,8411,8412,8413,8414,8417,8425,8429,8433,8437,8440,8447,8455,8462,8463,8468,8469,8470,8484,8485,8489,8501,8558,8559,8560,8567,8569,8570,8571,8572,8578,8584,8585,8586,8587,8588,8591,8593,8600,8601,8604,8610,8939,8940,8941,8943,8944,8945,8947,8949,8951,8953,8954,8955,8957,8968,8969,8970,8971,8972,8973,8976,8978,8984,8987,8990,8993,9014,9034,9035,9036,9037,9038,9039,9041,9042,9043,9044,9045,9046,9047],[0.0010232165014845865,0.020081487844659792,0.006499353779055165,0.007081376443157888,7.3628090995095345e-06,1.2572177618992374e-05,0.000134713763785803,4.262499033561051e-05,0.003916354843590017,0.0009945265819977518,0.00017990126304076668,0.0006109417036633956,0.0002417381752570674,0.0011710288815462093,0.01837942619521665,9.290358113566855e-05,9.85120139523495e-05,0.00033660560277973294,1.507640613491103e-06,3.4566468313065994e-05,1.0610584351391075e-05,2.9487741593269087e-05,7.25385616374768e-05,5.7315949076739435e-05,7.669852123138802e-06,0.009968214216581419,3.5278790355691026e-05,0.05069350620400212,0.0017964606639670354,0.0041134711520541,0.0005545755188426508,0.01326660124983864,0.000484956996692958,0.00016100612483374268,0.003446104514341906,0.0008723209552633905,0.003743753361631887,0.0005353150973401561,0.0015490663081172667,0.0002202726912899005,0.01208234148854962,0.00012794377480185477,0.0017322968562504244,5.474716969201334e-05,1.8342162997607265e-06,0.00033802279287897666,0.0017596724093966861,0.0013859705114328091,5.6490429309886295e-05,0.0003150413073470361,0.00012409626401027013,0.00039410488918457167,3.493646613504235e-05,2.3517544597694867e-05,2.8847256319125355e-05,0.00015430371704462108,4.328450648288299e-05,2.1384036216040082e-05,2.6860766667769465e-05,0.0001148917161708639,4.326520488223114e-05,0.0001284744281046466,3.961787689526903e-05,4.5507646832043293e-05,6.799435315806906e-05,4.3893299309106025e-05,0.000733270630602078,0.00014492120873970063,0.00236059191579475,0.019778758284387257,4.9371759343104405e-05,0.00024399357029254406,0.0001204459707258808,3.9058526347139096e-05,3.21503463910514e-05,0.00011666607499113511,0.00017341582269665668,7.859146274058127e-05,0.00020864070920285728,4.02366879902463e-05,0.00010113972498506315,0.0010355851176678135,3.0227794117752436e-05,0.003134792700137758,4.592892691694118e-05,4.0791903198925925e-05,7.714710679424471e-05,4.4613827888746e-05,8.405606556025438e-06,3.0019775244046352e-05,7.701104316631374e-05,9.506784547241085e-05,0.0001771799924875072,7.252365026930494e-05,5.328408000038324e-06,1.247396446083895e-05,0.00012350159251799608,0.00024363921854550036,0.0001465976343849101,0.0008247470418474227,0.0007847586155246211,4.506604717954584e-05,1.5343402892334592e-05,3.096388933926045e-05,2.7642515031793903e-05,3.479657801300691e-05,7.940659630190117e-06,1.498267839702275e-05,2.4353774999530597e-05,1.117510130492319e-05,0.0007324332140065361,0.13611891366803794,0.033960672247761385,0.022756026938719667,0.000360938286004534,7.073184047060445e-06,2.189274866027267e-05,5.857226377683809e-05,0.0005687417585942932,6.188442991229586e-05,0.000495585818115283,0.002986660793391849,8.808080041667761e-06,8.866543753106278e-06,0.00022789487334302184,0.0018215941487885262,5.4229849259311576e-06,0.0011798153171807295,0.016374606797646882,2.971249379919713e-05,0.0006491216022872511,0.0011681772800516083,0.0001240415374282391,0.005468357098205645,0.00019910519486762644,0.002499653942204821,0.0003741213448523682,0.0031440388727432596,0.000346409306817858,0.0029729303620878484,0.005249094619153373,6.757288039756026e-06,0.0013524001342965814,0.00018586093544943795,0.04769351508922645,0.020390844950952464,0.00109509821507466,0.006528355314573233,0.002684239476518891,0.0005101430797266074,0.0001289983034208833,0.003883745140413871,8.790452886181273e-05,1.4517989919106299e-05,1.6553617306642647e-05,6.244448049482556e-05,0.000830465252961986,0.0011390418840043627,3.286120906486239e-05,6.588063576414352e-05,3.7739530967038144e-06,2.104237769721599e-05,7.841458415869498e-05,1.1358900297097657e-05,0.0006604612349768835,3.638872571992399e-05,3.220594228054898e-05,0.00010577497760495147,4.5351563962952266e-05,2.483968742490824e-05,4.224923828242733e-05,2.4893335750061878e-05,4.46843350738111e-05,3.793606685088313e-05,3.2589475860468967e-06,4.253061717569094e-05,0.0032055466574914585,0.012579978500510779,0.018326979172682927,6.686238775879983e-05,0.00045582407276816505,0.0003771396437724862,0.00033489995809722004,0.0009658518144035084,0.0004122099727522067,5.163462494782041e-05,5.302336061622035e-06,2.6437899031284942e-05,9.130191996561677e-06,0.00017822030667105887,8.496039298879084e-07,6.556224004780032e-05,5.497865793795029e-06,0.0003484869457394903,7.472228774603023e-05,9.048913545152926e-05,0.0002546468858168474,6.026265667073128e-05,1.5174335617106312e-05,0.0025399554343561487,0.00014093476590044897,3.3249857777094085e-05,0.00047632147041114524,0.006917569138915354,3.048016688194632e-06,0.0017540403702986792,7.458206318040756e-06,0.00038722630913386066,0.0067556129479294,0.0010374309912370599,3.5876297307061677e-07,0.004536113248770392,7.242694935023061e-07,0.0002944824933270974,0.0005228953568956835,8.037954313623424e-05,0.0001527208451755022,0.0028122720350950746,3.4040473583731896e-06,4.1960322436406545e-05,0.0016181535605673777,3.7366129572908854e-05,0.011006251009399686,0.000485020994182228,3.812869147540774e-05,0.018484897037733972,0.02772013015405671,7.298625130648989e-06,0.00023444429584432714,4.6465573938715296e-05,8.257638439670108e-05,5.242650548171398e-05,1.5888361441089058e-05,0.0008339780843594453,0.0029667765313078232,0.003581353161397694,0.0029882245357981648,0.015449272127237798,0.052609950036846514,2.7251888907398184e-06,0.012096122783371339,0.0008968759232623015,0.0012897964929083354,0.0006142128745037026,0.0002038329343929412,5.867213582179361e-06,5.225790228360939e-06,4.374954261606192e-05,0.028572903610587057,0.07130883337253649,0.004615231962597029,0.0007093769021020793,0.00046472372915496677,5.1408290987356796e-05,2.3314135021156718e-05,0.0001072993146620711,0.0001836146545689052,0.00023250381438426552,0.0006371280078903873,0.02602051207219571,0.11614786406536098])\n"
     ]
    }
   ],
   "source": [
    "#rand_feat_imp = rand_cvmodel.bestModel.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtractFeatureImp(rand_cvmodel.bestModel.stages[-1].featureImportances, rand_train_predictions, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''''\n",
    "from itertools import chain\n",
    "attrs = sorted(\n",
    "    (attr[\"idx\"], attr[\"name\"]) for attr in (chain(*rand_train_predictions\n",
    "        .schema[\"features\"]\n",
    "        .metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "\n",
    "rand_feat_imp = [(name, rand_cvmodel.bestModel.stages[-1].featureImportances[idx])\n",
    " for idx, name in attrs\n",
    " if rand_cvmodel.bestModel.stages[-1].featureImportances[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassificationModel' object has no attribute 'pValues'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-bb4cae605980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_cvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpValues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-bb4cae605980>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_cvmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpValues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassificationModel' object has no attribute 'pValues'"
     ]
    }
   ],
   "source": [
    "[(name, rand_cvmodel.bestModel.stages[-1].pValues[idx]) for idx, name in attrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmlspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7fc87416346f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmmlspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmlspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmlspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark lightGBM\") \\\n",
    "            .config(\"spark.jars.packages\", \"Azure:mmlspark:0.17\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoint/')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import mmlspark\n",
    "from mmlspark import LightGBMClassifier\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from FeatureImportanceSelector import ExtractFeatureImp, FeatureImpSelector\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "sampling_seed=1111\n",
    "\n",
    "# The index of string values multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c),handleInvalid=\"keep\")\n",
    "    for c in stringCols\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "lgbm = (LightGBMClassifier(labelCol=\"label\", featuresCol=\"features\"))\n",
    "      \n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")   \n",
    "\n",
    "#labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "#                                   labels=labelIndexer.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler]+[lgbm])\n",
    "\n",
    "estimatorParam = ParamGridBuilder() \\\n",
    "    .addGrid(lgbm.numLeaves, [100,150]) \\\n",
    "    .addGrid(lgbm.maxDepth, [5,7]) \\\n",
    "    #.addGrid(lgbm.bagging_fraction, [.8]) \\\n",
    "    #.addGrid(lgbm.max_bin, [50]) \\\n",
    "    #.addGrid(lgbm.bagging_freq, [10]) \\\n",
    "    .addGrid(lgbm.earlyStoppingRound, [5]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "  \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3,\n",
    "                         parallelism=8,\n",
    "                         seed=sampling_seed)\n",
    "\n",
    "lgbm_cvmodel = crossval.fit(trainingData)      \n",
    "# Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n",
    "# is areaUnderROC.\n",
    "\n",
    "lgbm_path = temp_path + \"/lgbm\"\n",
    "lgbm.save(lgbm_path)\n",
    "model_path = temp_path + \"/lgbm_model\"\n",
    "lgbm_cvmodel.bestModel.save(model_path)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''2nd place lightGBM\n",
    "best_hyp = {'boosting_type': 'gbdt',\n",
    "            'class_weight': None,\n",
    "            'colsample_bytree': 0.2685319471585845,\n",
    "            'learning_rate': 0.011114918157721438,\n",
    "            'min_child_samples': 270,\n",
    "            'num_leaves': 261,\n",
    "            'reg_alpha': 0.4182767807212193,\n",
    "            'reg_lambda': 0.07336659149142766,\n",
    "            'subsample_for_bin': 40000,\n",
    "            'subsample': 0.6462594904717385}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Loading model\n",
    "from pyspark.ml import *\n",
    "lgbm_model_path = temp_path + \"/lgbm_model\"\n",
    "lgbm_mod2 = PipelineModel.load(lgbm_model_path)\n",
    "lgbm_predictions = lgbm_mod2.transform(testData)\n",
    "lgbm_train_predictions = lgbm_mod2.transform(trainingData)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(lgbm_train_predictions)))\n",
    "print(\"The area under ROC for test set is {}\".format(evaluator.evaluate(lgbm_predictions)))\n",
    "print ('Best maxDepth: ', lgbm_mod2.stages[-1]._java_obj.getMaxDepth()\n",
    "print ('Best maxBins: ', lgbm_mod2.stages[-1]._java_obj.getMaxBins())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 + Pyspark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "Pyspark+rev9+working+RF+and+LR+SEB",
  "notebookId": 4328943510894617
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
