{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "# LARGE, IMBALANCED dataset\n#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_reduced_reweighted_final_3.csv/part-00000-54f91216-357c-4161-9d25-ee9577740a9c-c000.csv\"\n# SMALL, BALANCED dataset\nfile_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv/part-00000-0b2544e2-78ed-44d9-808d-1d2e4ec6a8df-c000.csv\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>25</td><td>application_1560963602550_0019</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-mie162.xfypiqvfjzcexb0p1eqxrltidh.cx.internal.cloudapp.net:8088/proxy/application_1560963602550_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn15-mie162.xfypiqvfjzcexb0p1eqxrltidh.cx.internal.cloudapp.net:30060/node/containerlogs/container_e03_1560963602550_0019_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n/usr/bin/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 44.879150390625, "end_time": 1561000936546.365}}, "collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('abc_learn1').getOrCreate()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 1281.652099609375, "end_time": 1561000937845.766}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 11421.865966796875, "end_time": 1560990835897.446}}, "collapsed": false}}, {"execution_count": 3, "cell_type": "code", "source": "# Import the data into a Spark DataFrame with the schema \ndata = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\")\\\n.load(file_location)\n\n# Import libraries\nfrom pyspark.sql.functions import isnan, when, count, col\n\n#data = spark.read.csv(file_location, header=True, inferSchema=True)\n#data = (data.drop(\"Census_FirmwareVersionIdentifier\")\n#    .withColumn(\"AVProductsEnabled\", when(data[\"AVProductsEnabled\"]==\"unknown\", -1).otherwise(data[\"AVProductsEnabled\"]))\n#    .withColumn(\"classWeightCol\", when(data[\"classWeightCol\"]>.5, 0.091).otherwise(0.908)))\n\nfrom pyspark.sql.types import DoubleType, StringType, IntegerType\n\ndata_1 = (data.withColumn(\"AVProductsEnabled\", data[\"AVProductsEnabled\"].cast(IntegerType()))\n          .withColumn(\"AvSigVersion_new\", data[\"AvSigVersion_new\"].cast(StringType()))\n          .withColumn(\"Census_OSBuildNumber\", data[\"Census_OSBuildNumber\"].cast(StringType()))\n          .withColumn(\"Census_OSBuildRevision\", data[\"Census_OSBuildRevision\"].cast(StringType()))\n          .withColumn(\"Census_OSUILocaleIdentifier\", data[\"Census_OSUILocaleIdentifier\"].cast(StringType()))\n          .withColumn(\"Census_OSVersion_new\", data[\"Census_OSVersion_new\"].cast(StringType()))\n          .withColumn(\"CountryIdentifier\", data[\"CountryIdentifier\"].cast(StringType()))\n          .withColumn(\"LocaleEnglishNameIdentifier\", data[\"LocaleEnglishNameIdentifier\"].cast(StringType()))\n          .withColumn(\"OsBuild\", data[\"OsBuild\"].cast(StringType()))\n          .withColumn(\"OsSuite\", data[\"OsSuite\"].cast(StringType())))\ndata_1 = data_1.withColumnRenamed(\"HasDetections\",\"label\")\n#sampling_seed=1111\n#data = data_3.sampleBy(\"HasDetections\", fractions={0: .025, 1: .25}, seed=sampling_seed)\n#data_reduced.groupBy(\"HasDetections\").count().show()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 13342.176025390625, "end_time": 1561000951201.632}}, "collapsed": false}}, {"execution_count": 4, "cell_type": "code", "source": "#data_reduced.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv\")", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 40.47021484375, "end_time": 1561000951252.771}}, "collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_reduced_reweighted_final_3.csv/part-00000-54f91216-357c-4161-9d25-ee9577740a9c-c000.csv\"\n\n#file_location = \"wasbs:///mie1628-2019-06-18t21-07-05-930z@mie1628.blob.core.windows.net/MIE1628/data_SMALL_balanced.csv/part-00000-0b2544e2-78ed-44d9-808d-1d2e4ec6a8df-c000.csv\"\n\n#data = spark.read.csv(file_location, header=True, inferSchema=True)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 32.953857421875, "end_time": 1561000951348.299}}, "collapsed": true}}, {"execution_count": 6, "cell_type": "code", "source": "data_1.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['MachineIdentifier', 'EngineVersion', 'AppVersion', 'RtpStateBitfield', 'IsSxsPassiveMode', 'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled', 'HasTpm', 'CountryIdentifier', 'LocaleEnglishNameIdentifier', 'Platform', 'Processor', 'OsBuild', 'OsSuite', 'OsPlatformSubRelease', 'SkuEdition', 'IsProtected', 'Firewall', 'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_ProcessorCoreCount', 'Census_ProcessorManufacturerIdentifier', 'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName', 'Census_SystemVolumeTotalCapacity', 'Census_HasOpticalDiskDrive', 'Census_TotalPhysicalRAM', 'Census_ChassisTypeName', 'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal', 'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSEdition', 'Census_OSSkuName', 'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSWUAutoUpdateOptionsName', 'Census_IsPortableOperatingSystem', 'Census_GenuineStateName', 'Census_ActivationChannel', 'Census_FlightRing', 'Census_FirmwareManufacturerIdentifier', 'Census_IsSecureBootEnabled', 'Census_IsVirtualDevice', 'Census_IsTouchEnabled', 'Census_IsPenCapable', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 'Wdft_RegionIdentifier', 'label', 'classWeightCol', 'AvSigVersion_new', 'OSVer_new', 'Census_OSVersion_new', 'OsBuildLab_new']"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 240.60400390625, "end_time": 1561000951599.603}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.52490234375, "end_time": 1560984672415.929}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 29.470947265625, "end_time": 1560984672456.706}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 244.327880859375, "end_time": 1560958443914.72}}, "collapsed": true}}, {"execution_count": 7, "cell_type": "code", "source": "### Number of nulls or NAN in each column\n#numOfnulls = data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns])\n### View the number of nulls in each column in a Pandas DataFrame format\n#numOfnulls_pandas = numOfnulls.toPandas()\n#numOfnulls_pandas.T", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 32.26123046875, "end_time": 1561000951645.803}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 238.85693359375, "end_time": 1560960831272.571}}, "collapsed": true}}, {"execution_count": 8, "cell_type": "code", "source": "# This function drops columns which have more than 10% of their entries as null values\nimport pyspark.sql.functions as F\n\ndef drop_null_columns(df):\n    \"\"\"\n    This function drops columns which have more than 10% of their entries as null values.\n    :param df: A PySpark DataFrame\n    \"\"\"\n    null_counts = df.select([F.count(F.when(isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n    to_drop = [k for k, v in null_counts.items() if v > 0.1*(df.count())]\n    df = df.drop(*to_drop)\n    return df", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.934814453125, "end_time": 1561000951691.302}}, "collapsed": true}}, {"execution_count": 9, "cell_type": "code", "source": "# Drop the colums with more than 10% of the records containing and create a new DataFrame \"data_1\"\n#data_1 = drop_null_columns(data)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 32.364013671875, "end_time": 1561000951735.407}}, "collapsed": true}}, {"execution_count": 10, "cell_type": "code", "source": "# Check the number of columns remaining in DF \"data_1\"\n#len(data_1.columns)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 38.991943359375, "end_time": 1561000951785.969}}, "collapsed": false}}, {"execution_count": 11, "cell_type": "code", "source": "##### Check the number of unique values in each column of the dataframe \"data_1\" and view in a Pandas DataFrame format\n#numUniqueVals = data_1.agg(*(F.countDistinct(col(c)).alias(c) for c in data_1.columns))\n#numUniqueVals_pandas = numUniqueVals.toPandas()\n# numUniqueVals_pandas.T", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 41.965087890625, "end_time": 1561000951839.131}}, "collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "#### Create a list of columns which have less than 20 unique values\n#numUniqueVals_pandas.T\n# Cols_with_less_than_20Unique_vals = [col for col, val in numUniqueVals_pandas.iteritems() if numUniqueVals_pandas[col][0]<20 ]\n# Cols_with_less_than_20Unique_vals", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 34.046142578125, "end_time": 1561000951883.991}}, "collapsed": false}}, {"execution_count": 13, "cell_type": "code", "source": "# Create a new DF \"data_2\" which contains the columns with less than 20 unique values only\n#data_2 = data_1.select(Cols_with_less_than_20Unique_vals)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 29.566162109375, "end_time": 1561000951923.976}}, "collapsed": true}}, {"execution_count": 14, "cell_type": "code", "source": "# Check the number of columns in DF \"data_2\"\n#len(data_2.columns)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 31.921875, "end_time": 1561000951966.616}}, "collapsed": false}}, {"execution_count": 15, "cell_type": "code", "source": "# Sanity check: Check that the number of samples (rows) has not changed from the original dataset ", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 38.18408203125, "end_time": 1561000952015.173}}, "collapsed": false}}, {"execution_count": 16, "cell_type": "code", "source": "# Drop the rows (records) with NA entries in any of the X columns of \"data_2\" and save into a new DF \"data_3\"\ndata_1 = data_1.dropna(how='any')", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 244.998046875, "end_time": 1561000952272.835}}, "collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "# Check the number of rows remaining in DF \"data_1\"\ndata_1.select(\"label\").count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "224119"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 3272.69189453125, "end_time": 1561000955556.591}}, "collapsed": false}}, {"execution_count": 18, "cell_type": "code", "source": "# Create a sample of the data to look at in a Pandas dataframe (the sample contains 5% of the full data)\n# data_sample = data_3.sample(False, 0.005)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.3359375, "end_time": 1561000955596.428}}, "collapsed": true}}, {"execution_count": 19, "cell_type": "code", "source": "# Drop the column containing the labels from the features dataframe before vectorizing\ndata_1b = data_1.drop(\"label\").drop(\"MachineIdentifier\")", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 241.799072265625, "end_time": 1561000955850.619}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 34.00390625, "end_time": 1560990139813.923}}, "collapsed": true}}, {"execution_count": 20, "cell_type": "code", "source": "# List the data types of the columns in DF \"data_3b\"\ndata_1.dtypes", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('MachineIdentifier', 'string'), ('EngineVersion', 'string'), ('AppVersion', 'string'), ('RtpStateBitfield', 'string'), ('IsSxsPassiveMode', 'int'), ('AVProductStatesIdentifier', 'string'), ('AVProductsInstalled', 'int'), ('AVProductsEnabled', 'int'), ('HasTpm', 'int'), ('CountryIdentifier', 'string'), ('LocaleEnglishNameIdentifier', 'string'), ('Platform', 'string'), ('Processor', 'string'), ('OsBuild', 'string'), ('OsSuite', 'string'), ('OsPlatformSubRelease', 'string'), ('SkuEdition', 'string'), ('IsProtected', 'string'), ('Firewall', 'string'), ('Census_MDC2FormFactor', 'string'), ('Census_DeviceFamily', 'string'), ('Census_ProcessorCoreCount', 'double'), ('Census_ProcessorManufacturerIdentifier', 'string'), ('Census_PrimaryDiskTotalCapacity', 'int'), ('Census_PrimaryDiskTypeName', 'string'), ('Census_SystemVolumeTotalCapacity', 'double'), ('Census_HasOpticalDiskDrive', 'int'), ('Census_TotalPhysicalRAM', 'double'), ('Census_ChassisTypeName', 'string'), ('Census_InternalPrimaryDiagonalDisplaySizeInInches', 'double'), ('Census_InternalPrimaryDisplayResolutionHorizontal', 'double'), ('Census_InternalPrimaryDisplayResolutionVertical', 'double'), ('Census_PowerPlatformRoleName', 'string'), ('Census_OSArchitecture', 'string'), ('Census_OSBranch', 'string'), ('Census_OSBuildNumber', 'string'), ('Census_OSBuildRevision', 'string'), ('Census_OSEdition', 'string'), ('Census_OSSkuName', 'string'), ('Census_OSInstallTypeName', 'string'), ('Census_OSInstallLanguageIdentifier', 'string'), ('Census_OSUILocaleIdentifier', 'string'), ('Census_OSWUAutoUpdateOptionsName', 'string'), ('Census_IsPortableOperatingSystem', 'int'), ('Census_GenuineStateName', 'string'), ('Census_ActivationChannel', 'string'), ('Census_FlightRing', 'string'), ('Census_FirmwareManufacturerIdentifier', 'string'), ('Census_IsSecureBootEnabled', 'int'), ('Census_IsVirtualDevice', 'string'), ('Census_IsTouchEnabled', 'int'), ('Census_IsPenCapable', 'int'), ('Census_IsAlwaysOnAlwaysConnectedCapable', 'string'), ('Wdft_IsGamer', 'string'), ('Wdft_RegionIdentifier', 'string'), ('label', 'int'), ('classWeightCol', 'double'), ('AvSigVersion_new', 'string'), ('OSVer_new', 'string'), ('Census_OSVersion_new', 'string'), ('OsBuildLab_new', 'string')]"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 35.260986328125, "end_time": 1561000955896.614}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 244.569091796875, "end_time": 1560961084987.638}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 123744.0849609375, "end_time": 1560962700205.858}}, "collapsed": false}}, {"execution_count": 21, "cell_type": "code", "source": "stringCols = []\nfor col in data_1.dtypes:\n    if col[1] == 'string':\n        stringCols.append(col[0])", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 29.427978515625, "end_time": 1561000955937.516}}, "collapsed": true}}, {"execution_count": 22, "cell_type": "code", "source": "stringCols.pop(0)\nstringCols", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['EngineVersion', 'AppVersion', 'RtpStateBitfield', 'AVProductStatesIdentifier', 'CountryIdentifier', 'LocaleEnglishNameIdentifier', 'Platform', 'Processor', 'OsBuild', 'OsSuite', 'OsPlatformSubRelease', 'SkuEdition', 'IsProtected', 'Firewall', 'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_ProcessorManufacturerIdentifier', 'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName', 'Census_PowerPlatformRoleName', 'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSBuildNumber', 'Census_OSBuildRevision', 'Census_OSEdition', 'Census_OSSkuName', 'Census_OSInstallTypeName', 'Census_OSInstallLanguageIdentifier', 'Census_OSUILocaleIdentifier', 'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName', 'Census_ActivationChannel', 'Census_FlightRing', 'Census_FirmwareManufacturerIdentifier', 'Census_IsVirtualDevice', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 'Wdft_RegionIdentifier', 'AvSigVersion_new', 'OSVer_new', 'Census_OSVersion_new', 'OsBuildLab_new']"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 28.079833984375, "end_time": 1561000955976.229}}, "collapsed": false}}, {"execution_count": 23, "cell_type": "code", "source": "import numpy as np\nnumericCols = np.setdiff1d(data_1.columns, stringCols).tolist()\nnumericCols.remove(\"MachineIdentifier\")\nnumericCols.remove(\"classWeightCol\")\nnumericCols.remove(\"label\")", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 241.422119140625, "end_time": 1561000956234.665}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 32.5390625, "end_time": 1560998512945.661}}, "collapsed": false}}, {"execution_count": 24, "cell_type": "code", "source": "data_1c = data_1.select(stringCols).drop(\"MachineIdentifier\")", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 238.7509765625, "end_time": 1561000956484.532}}, "collapsed": false}}, {"source": "#### Encode categorical features using StringIndexer", "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "# Import libraries\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 2265.387939453125, "end_time": 1561000958762.941}}, "collapsed": true}}, {"execution_count": 26, "cell_type": "code", "source": "# Use StringIndexer to create numerical categorical columns for the columns of dataFrame \"data_3b\"\n#indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(data_1c) \\\n#            for column in data_1c.columns]\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n\n# The index of string values multiple columns\nindexers = [\n    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n    for c in stringCols\n]\n\n# The encode of indexed vlaues multiple columns\nencoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n    for indexer in indexers\n]\n\n# Vectorizing encoded values\nassembler = VectorAssembler(inputCols=([encoder.getOutputCol() for encoder in encoders] + numericCols),outputCol=\"features\")\n\npipeline = Pipeline(stages=indexers + encoders+[assembler])\nmodel=pipeline.fit(data_1)\ntransformed = model.transform(data_1)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 69635.3779296875, "end_time": 1561001028408.201}}, "collapsed": false}}, {"execution_count": 27, "cell_type": "code", "source": "transformed.select(\"features\").show(1)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+\n|            features|\n+--------------------+\n|(5351,[1,49,128,1...|\n+--------------------+\nonly showing top 1 row"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 2261.166015625, "end_time": 1561001030679.331}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.139892578125, "end_time": 1560993392429.089}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.030029296875, "end_time": 1560993396864.244}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 238.296875, "end_time": 1560993399774.27}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 28.50390625, "end_time": 1560992874259.554}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 29.869873046875, "end_time": 1560993403645.71}}, "collapsed": false}}, {"source": "### One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. This encoding allows algorithms which expect continuous features, such as Logistic Regression, to use categorical features. For string type input data, it is common to encode categorical features using StringIndexer first (done above).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 31.87109375, "end_time": 1560992995436.11}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 79.52197265625, "end_time": 1560993406649.388}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 746.5830078125, "end_time": 1560993004652.667}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 245.781982421875, "end_time": 1560993007432.253}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 42.3837890625, "end_time": 1560993011556.415}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.56884765625, "end_time": 1560993014102.464}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 237.83984375, "end_time": 1560993018430.3}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.218017578125, "end_time": 1560984772671.123}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 40.31884765625, "end_time": 1560984772722.694}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 31.658203125, "end_time": 1560984772765.784}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 237.30908203125, "end_time": 1560993025464.336}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 38.596923828125, "end_time": 1560993280853.363}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.964111328125, "end_time": 1560993285580.439}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 778.48388671875, "end_time": 1560993359210.415}}, "collapsed": false}}, {"execution_count": 28, "cell_type": "code", "source": "#data_5b.columns", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 28.983154296875, "end_time": 1561001030720.243}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 241.389892578125, "end_time": 1560984775884.641}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 36.2958984375, "end_time": 1560984775932.251}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 744.4189453125, "end_time": 1560984776689.356}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 752.427001953125, "end_time": 1560984777453.685}}, "collapsed": false}}, {"execution_count": 29, "cell_type": "code", "source": "# Create a dataFrame called \"final_df_1\" composed of two columns:\n# the first column is called \"features\" and it includes the feature vector for each record (which is a vector produced by one hot encoding then vector assembler)\n# the second column is the \"Has detection\" column which is basically the label for each record \ndataset = transformed.select('features','label','classWeightCol')\ndataset.columns", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['features', 'label', 'classWeightCol']"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 242.0390625, "end_time": 1561001030974.388}}, "collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "#dataset = final_df_1.crossJoin(data_3.select(\"HasDetections\")).crossJoin(data_3.select(\"classWeightCol\"))\n", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 31.6171875, "end_time": 1561001031020.83}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 241.19482421875, "end_time": 1560984778806.989}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 239.492919921875, "end_time": 1560981843648.328}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 7274.823974609375, "end_time": 1560981850934.409}}, "collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "#dataset.count()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.132080078125, "end_time": 1561001031064.611}}, "collapsed": true}}, {"execution_count": 32, "cell_type": "code", "source": "# Create a sample of the data to look at in a Pandas dataframe (the sample contains 5% of the full data)\n# dataset_sample = dataset.sample(False, 0.02)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 47.50390625, "end_time": 1561001031123.179}}, "collapsed": true}}, {"execution_count": 33, "cell_type": "code", "source": "# dataset_sample.count()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 28.9619140625, "end_time": 1561001031167.516}}, "collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "# dataset_sample.show(5)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.679931640625, "end_time": 1561001031216.644}}, "collapsed": true}}, {"execution_count": 35, "cell_type": "code", "source": "#final_df_3.write.save('/user/mie_aabdelmageed/final_df_3', format='parquet', mode='append')", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 34.02197265625, "end_time": 1561001031261.655}}, "collapsed": true}}, {"execution_count": 36, "cell_type": "code", "source": "#Good = dataset.filter(dataset.HasDetections == 0)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 34.89501953125, "end_time": 1561001031308.226}}, "collapsed": true}}, {"execution_count": 37, "cell_type": "code", "source": "#Good.show(5)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 28.27001953125, "end_time": 1561001031347.351}}, "collapsed": false}}, {"execution_count": 38, "cell_type": "code", "source": "#Good.count()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 29.619140625, "end_time": 1561001031391.172}}, "collapsed": true}}, {"execution_count": 39, "cell_type": "code", "source": "#noGood = dataset.filter(dataset.HasDetections == 1)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 33.31298828125, "end_time": 1561001031435.425}}, "collapsed": true}}, {"execution_count": 40, "cell_type": "code", "source": "#noGood.count()", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 36.14599609375, "end_time": 1561001031483.321}}, "collapsed": true}}, {"source": "## Stratified Split", "cell_type": "markdown", "metadata": {}}, {"execution_count": 41, "cell_type": "code", "source": "sampling_seed=1111\ntrainingData = dataset.sampleBy(\"label\", fractions={0: .9, 1: .9}, seed=sampling_seed)\n\n# Subtracting 'train' from original 'data' to get test set \ntestData = dataset.subtract(trainingData)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 761.987060546875, "end_time": 1561001032258.611}}, "collapsed": false}}, {"execution_count": 42, "cell_type": "code", "source": "trainingData.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----+--------------+\n|            features|label|classWeightCol|\n+--------------------+-----+--------------+\n|(5351,[1,65,128,1...|    1|         0.908|\n|(5351,[1,54,128,1...|    1|         0.908|\n|(5351,[9,48,128,3...|    0|         0.091|\n|(5351,[2,57,128,1...|    0|         0.091|\n|(5351,[0,54,128,1...|    1|         0.908|\n+--------------------+-----+--------------+\nonly showing top 5 rows"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 3286.965087890625, "end_time": 1561001035558.115}}, "collapsed": false}}, {"execution_count": 43, "cell_type": "code", "source": "from pyspark.ml.classification import LogisticRegression", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 238.0458984375, "end_time": 1561001035807.61}}, "collapsed": true}}, {"execution_count": 44, "cell_type": "code", "source": "### Randomly split data into training and test sets. set seed for reproducibility\n#(trainingData, testData) = dataset_sample.randomSplit([0.8, 0.2], seed=100)\n#print(trainingData.count())\n#print(testData.count())", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 28.193115234375, "end_time": 1561001035845.997}}, "collapsed": true}}, {"execution_count": 45, "cell_type": "code", "source": "# Create initial LogisticRegression model\nlr = (LogisticRegression(labelCol=\"label\", featuresCol=\"features\",\n                         maxIter=100, regParam=0.1, elasticNetParam=0.5))\n                        \n#lr = LogisticRegression(labelCol=\"HasDetections\", featuresCol=\"features\", weightCol=\"classWeightCol\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 75298.54296875, "end_time": 1561001111154.45}}, "collapsed": false}}, {"execution_count": 46, "cell_type": "code", "source": "# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 739.402099609375, "end_time": 1561001111905.429}}, "collapsed": true}}, {"execution_count": 47, "cell_type": "code", "source": "predictions.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----+--------------+--------------------+--------------------+----------+\n|            features|label|classWeightCol|       rawPrediction|         probability|prediction|\n+--------------------+-----+--------------+--------------------+--------------------+----------+\n|(5351,[2,56,128,1...|    1|         0.908|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[1,48,128,1...|    1|         0.908|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[0,48,129,2...|    0|         0.091|[0.14625370819804...|[0.53649839140106...|       0.0|\n|(5351,[0,48,128,1...|    0|         0.091|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[1,50,128,1...|    1|         0.908|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[6,79,128,1...|    1|         0.908|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[0,57,128,1...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[24,68,128,...|    1|         0.908|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[0,48,128,1...|    0|         0.091|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[0,55,128,1...|    1|         0.908|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[3,63,128,1...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[1,48,128,1...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[1,57,128,1...|    1|         0.908|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[2,52,128,1...|    0|         0.091|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[3,58,128,5...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[10,63,128,...|    1|         0.908|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[6,74,128,1...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[0,48,128,3...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n|(5351,[0,48,128,1...|    0|         0.091|[-0.0642615610951...|[0.48394013601111...|       1.0|\n|(5351,[1,48,128,1...|    0|         0.091|[0.12929520077493...|[0.53227884492719...|       0.0|\n+--------------------+-----+--------------+--------------------+--------------------+----------+\nonly showing top 20 rows"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 17452.248046875, "end_time": 1561001129370.044}}, "collapsed": false}}, {"execution_count": 48, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.5724250595964253"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 15948.516845703125, "end_time": 1561001145330.542}}, "collapsed": false}}, {"execution_count": 49, "cell_type": "code", "source": "####Cross Validation", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.29296875, "end_time": 1561001145372.635}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"source": "### Feature Selection using ChiSqSelector\nChiSqSelector stands for Chi-Squared feature selection. It operates on labeled data with categorical features. ChiSqSelector uses the Chi-Squared test of independence to decide which features to choose", "cell_type": "markdown", "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": "from pyspark.ml.feature import ChiSqSelector\n# from pyspark.ml.linalg import Vectors", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 30.827880859375, "end_time": 1561001145414.553}}, "collapsed": true}}, {"execution_count": 51, "cell_type": "code", "source": "# Create a chiSq selector instance to select the top 50 most important features in the dataset \"final_df_3\"\nselector_1 = ChiSqSelector(numTopFeatures=50, featuresCol=\"features\", \\\n                         outputCol=\"selectedFeatures\", labelCol=\"label\")", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 247.630126953125, "end_time": 1561001145674.193}}, "collapsed": true}}, {"execution_count": 52, "cell_type": "code", "source": "final_df_3_chi = selector_1.fit(dataset).transform(dataset)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "'Field \"HasDetections\" does not exist.\\nAvailable fields: features, label, classWeightCol'\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 288, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 285, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: 'Field \"HasDetections\" does not exist.\\nAvailable fields: features, label, classWeightCol'\n\n"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 235.4970703125, "end_time": 1561001145922.081}}, "collapsed": false}}, {"execution_count": 53, "cell_type": "code", "source": "final_df_3_chi.show(5)", "outputs": [{"output_type": "stream", "name": "stderr", "text": "name 'final_df_3_chi' is not defined\nTraceback (most recent call last):\nNameError: name 'final_df_3_chi' is not defined\n\n"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 36.5849609375, "end_time": 1561001145968.812}}, "collapsed": false}}, {"execution_count": 54, "cell_type": "code", "source": "# final_df_3_chi.write.save('/user/mie_aabdelmageed/final_df_3_chi', format='parquet', mode='append')", "outputs": [], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 241.1650390625, "end_time": 1561001146223.114}}, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"source": "## Gradient Boosted Trees", "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": "from pyspark.ml.classification import GBTClassifier\n\n\ngbt = GBTClassifier(labelCol=\"HasDetections\", featuresCol=\"features\", maxIter=10)\nGBT_Model = gbt.fit(trainingData)\ngbt_predictions = GBT_Model.transform(testData)\nevaluator = BinaryClassificationEvaluator()\n\n\nprint(\"Test_SET (Area Under ROC): \" + str(evaluator.evaluate(gbt_predictions, {evaluator.metricName: \"areaUnderROC\"})))\n", "outputs": [{"output_type": "stream", "name": "stderr", "text": "'Field \"HasDetections\" does not exist.\\nAvailable fields: features, label, classWeightCol'\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 288, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 285, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\npyspark.sql.utils.IllegalArgumentException: 'Field \"HasDetections\" does not exist.\\nAvailable fields: features, label, classWeightCol'\n\n"}], "metadata": {"scrolled": true, "cell_status": {"execute_time": {"duration": 239.802001953125, "end_time": 1561001146478.054}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}, "celltoolbar": "Slideshow"}}